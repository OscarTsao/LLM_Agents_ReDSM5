{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ReDSM5 LLM Fine-tuning on Google Colab\n",
    "\n",
    "This notebook enables training decoder-only LLMs (Llama/Qwen) with **TPU/GPU** acceleration on Google Colab.\n",
    "\n",
    "## Features\n",
    "- \u2705 Automatic TPU/GPU/CPU detection\n",
    "- \u2705 LoRA/QLoRA support for efficient fine-tuning\n",
    "- \u2705 Multi-label DSM-5 symptom classification\n",
    "- \u2705 Sliding window for long documents\n",
    "- \u2705 Threshold optimization and model export\n",
    "\n",
    "## Before Starting\n",
    "1. Go to **Runtime > Change runtime type**\n",
    "2. Select **T4 GPU** or **TPU v2** for hardware accelerator\n",
    "3. Click **Save**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Installation\n",
    "print(\"\ud83d\udce6 Installing dependencies...\")\n",
    "!pip install -q transformers>=4.36.0 datasets>=2.16.0 accelerate>=0.25.0\n",
    "!pip install -q peft>=0.7.0 bitsandbytes>=0.41.0 scipy scikit-learn\n",
    "!pip install -q wandb optuna pyyaml pandas matplotlib seaborn\n",
    "\n",
    "# For TPU support (optional - will be skipped if not on TPU)\n",
    "try:\n",
    "    !pip install -q cloud-tpu-client\n",
    "    !pip install -q torch-xla\n",
    "    print(\"\u2705 TPU libraries installed\")\n",
    "except:\n",
    "    print(\"\u26a0\ufe0f  TPU libraries not available (using GPU/CPU)\")\n",
    "\n",
    "# Clone repository\n",
    "import os\n",
    "if not os.path.exists('LLM_Agents_ReDSM5'):\n",
    "    !git clone https://github.com/OscarTsao/LLM_Agents_ReDSM5.git\n",
    "    print(\"\u2705 Repository cloned\")\n",
    "else:\n",
    "    print(\"\u2705 Repository already exists\")\n",
    "\n",
    "%cd LLM_Agents_ReDSM5\n",
    "print(\"\\n\u2705 Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hardware_detection"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Hardware Detection\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"\ud83d\udd0d Detecting hardware...\\n\")\n",
    "\n",
    "# Try TPU detection\n",
    "USE_TPU = False\n",
    "try:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "    USE_TPU = True\n",
    "    print(f\"\u2705 TPU detected: {device}\")\n",
    "    print(f\"   TPU cores: {xm.xrt_world_size()}\")\n",
    "except ImportError:\n",
    "    USE_TPU = False\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\u2705 GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"   Compute capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  CPU only - training will be slow\")\n",
    "        print(\"   Recommendation: Use Runtime > Change runtime type to enable GPU/TPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Environment Info:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"   TPU mode: {USE_TPU}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Mount Google Drive (Optional - for data/checkpoints)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "MOUNT_DRIVE = False  # Set to True to use Google Drive\n",
    "\n",
    "if MOUNT_DRIVE:\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = '/content/drive/MyDrive/redsm5_data'\n",
    "    OUTPUT_BASE = '/content/drive/MyDrive/redsm5_outputs'\n",
    "    print(f\"\u2705 Drive mounted\")\n",
    "    print(f\"   Data directory: {DATA_DIR}\")\n",
    "    print(f\"   Output directory: {OUTPUT_BASE}\")\n",
    "else:\n",
    "    print(\"\u2139\ufe0f  Using local storage (data will be lost after session ends)\")\n",
    "    print(\"   Set MOUNT_DRIVE=True to use Google Drive for persistent storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_data"
   },
   "outputs": [],
   "source": "# Cell 4: Load and Process ReDSM5 Data\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nprint(\"\ud83d\udcca Loading ReDSM5 Dataset...\\n\")\n\n# Option: Use synthetic data for quick testing (set to False for real training)\nUSE_SYNTHETIC_DATA = False  # Set to True only for quick testing\n\nif USE_SYNTHETIC_DATA:\n    print(\"\u26a0\ufe0f  Using SYNTHETIC data (for testing only)\")\n    print(\"   Set USE_SYNTHETIC_DATA=False to use real ReDSM5 data\\n\")\n    from tests.fixtures.data import generate_synthetic_dataset\n    DATA_DIR = Path('/content/sample_data')\n    generate_synthetic_dataset(DATA_DIR, num_samples=200, seed=42)\n    print(f\"\u2705 Generated {len(list(DATA_DIR.glob('*.jsonl')))} synthetic files\")\nelse:\n    print(\"\u2705 Using REAL ReDSM5 data from repository\\n\")\n    \n    # Symptom mapping from ReDSM5 to our label names\n    SYMPTOM_MAP = {\n        'DEPRESSED_MOOD': 'depressed_mood',\n        'ANHEDONIA': 'diminished_interest',\n        'APPETITE_CHANGE': 'weight_appetite_change',\n        'SLEEP_ISSUES': 'sleep_disturbance',\n        'PSYCHOMOTOR': 'psychomotor',\n        'FATIGUE': 'fatigue',\n        'WORTHLESSNESS': 'worthlessness_guilt',\n        'COGNITIVE_ISSUES': 'concentration_indecision',\n        'SUICIDAL_THOUGHTS': 'suicidality',\n    }\n    LABEL_NAMES = list(SYMPTOM_MAP.values())\n    \n    # Load posts\n    posts_df = pd.read_csv('data/redsm5/redsm5_posts.csv')\n    print(f\"\ud83d\udcc4 Loaded {len(posts_df)} posts\")\n    \n    # Load annotations\n    annot_df = pd.read_csv('data/redsm5/redsm5_annotations.csv')\n    print(f\"\ud83d\udcdd Loaded {len(annot_df)} sentence-level annotations\")\n    \n    # Filter positive annotations (status=1), exclude SPECIAL_CASE\n    positive_annot = annot_df[\n        (annot_df['status'] == 1) & \n        (annot_df['DSM5_symptom'] != 'SPECIAL_CASE')\n    ]\n    print(f\"   \u2192 {len(positive_annot)} positive symptom annotations\")\n    \n    # Create document-level labels (multi-label per post)\n    post_symptoms = positive_annot.groupby('post_id')['DSM5_symptom'].apply(set).to_dict()\n    \n    # Initialize all labels to 0\n    for label in LABEL_NAMES:\n        posts_df[label] = 0\n    \n    # Set to 1 where symptoms present\n    for post_id, symptoms in post_symptoms.items():\n        if post_id in posts_df['post_id'].values:\n            idx = posts_df[posts_df['post_id'] == post_id].index[0]\n            for symptom in symptoms:\n                if symptom in SYMPTOM_MAP:\n                    posts_df.loc[idx, SYMPTOM_MAP[symptom]] = 1\n    \n    # Print label distribution\n    print(f\"\\n\ud83d\udcca Label Distribution:\")\n    for label in LABEL_NAMES:\n        count = posts_df[label].sum()\n        pct = 100 * count / len(posts_df)\n        print(f\"   {label:30s}: {int(count):4d} ({pct:5.1f}%)\")\n    \n    posts_with_symptoms = (posts_df[LABEL_NAMES].sum(axis=1) > 0).sum()\n    print(f\"\\n   Total posts: {len(posts_df)}\")\n    print(f\"   Posts with \u22651 symptom: {posts_with_symptoms} ({100*posts_with_symptoms/len(posts_df):.1f}%)\")\n    \n    # Drop post_id, keep text and labels\n    posts_df = posts_df.drop(columns=['post_id'])\n    \n    # Stratified split: 70% train, 15% dev, 15% test\n    print(f\"\\n\ud83d\udd00 Splitting data (70/15/15)...\")\n    posts_df['label_sig'] = posts_df[LABEL_NAMES].apply(\n        lambda r: ''.join(str(int(v)) for v in r), axis=1\n    )\n    \n    # Train vs (dev+test)\n    train_df, temp_df = train_test_split(\n        posts_df, \n        train_size=0.7, \n        random_state=42,\n        stratify=posts_df['label_sig'] if posts_df['label_sig'].nunique() > 1 else None\n    )\n    \n    # Dev vs test\n    dev_df, test_df = train_test_split(\n        temp_df,\n        train_size=0.5,  # 50% of remaining = 15% of total\n        random_state=42,\n        stratify=temp_df['label_sig'] if temp_df['label_sig'].nunique() > 1 else None\n    )\n    \n    # Drop stratification column\n    for df in [train_df, dev_df, test_df]:\n        df.drop(columns=['label_sig'], inplace=True)\n    \n    print(f\"   Train: {len(train_df):4d} samples ({100*len(train_df)/len(posts_df):.1f}%)\")\n    print(f\"   Dev:   {len(dev_df):4d} samples ({100*len(dev_df)/len(posts_df):.1f}%)\")\n    print(f\"   Test:  {len(test_df):4d} samples ({100*len(test_df)/len(posts_df):.1f}%)\")\n    \n    # Save to JSONL\n    DATA_DIR = Path('/content/redsm5_processed')\n    DATA_DIR.mkdir(exist_ok=True)\n    \n    train_df.to_json(DATA_DIR / 'train.jsonl', orient='records', lines=True)\n    dev_df.to_json(DATA_DIR / 'dev.jsonl', orient='records', lines=True)\n    test_df.to_json(DATA_DIR / 'test.jsonl', orient='records', lines=True)\n    \n    print(f\"\\n\u2705 Saved processed data to {DATA_DIR}/\")\n    print(f\"   train.jsonl, dev.jsonl, test.jsonl\")\n\nprint(f\"\\n\ud83c\udfaf Data directory: {DATA_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_login"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Hugging Face Login (for gated models)\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "USE_GATED_MODEL = False  # Set to True if using Llama-2 or other gated models\n",
    "\n",
    "if USE_GATED_MODEL:\n",
    "    print(\"\ud83d\udd10 Please log in to Hugging Face...\")\n",
    "    notebook_login()\n",
    "    print(\"\u2705 Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"\u2139\ufe0f  Skipping HF login (not using gated models)\")\n",
    "    print(\"   Set USE_GATED_MODEL=True if using Llama-2 or similar models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Configure Training\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\u2699\ufe0f  Configuring training parameters...\\n\")\n",
    "\n",
    "# Choose model (use smaller models for faster experimentation)\n",
    "MODEL_OPTIONS = {\n",
    "    'tiny': 'hf-internal-testing/tiny-random-LlamaForSequenceClassification',  # For testing\n",
    "    'small': 'meta-llama/Llama-2-7b-hf',  # 7B model\n",
    "    'medium': 'Qwen/Qwen2.5-7B',  # Alternative 7B\n",
    "    'large': 'meta-llama/Llama-2-13b-hf'  # 13B model\n",
    "}\n",
    "\n",
    "MODEL_SIZE = 'tiny'  # Change to 'small', 'medium', or 'large' for production\n",
    "\n",
    "config = {\n",
    "    # Model settings\n",
    "    'model_id': MODEL_OPTIONS[MODEL_SIZE],\n",
    "    'method': 'lora',  # 'full_ft', 'lora', or 'qlora'\n",
    "    \n",
    "    # Training hyperparameters (optimized for TPU/GPU)\n",
    "    'num_train_epochs': 3,\n",
    "    'per_device_train_batch_size': 8 if USE_TPU else (4 if torch.cuda.is_available() else 2),\n",
    "    'per_device_eval_batch_size': 16 if USE_TPU else (8 if torch.cuda.is_available() else 4),\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    \n",
    "    # LoRA settings (if method='lora' or 'qlora')\n",
    "    'lora_r': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.05,\n",
    "    'lora_target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n",
    "    \n",
    "    # Document processing\n",
    "    'max_length': 2048 if USE_TPU else (1024 if torch.cuda.is_available() else 512),\n",
    "    'doc_stride': 512,\n",
    "    'truncation_strategy': 'window_pool',\n",
    "    'pooler': 'mean',  # 'max', 'mean', or 'logit_sum'\n",
    "    \n",
    "    # Loss settings\n",
    "    'loss_type': 'bce',  # 'bce' or 'focal'\n",
    "    'class_weighting': 'sqrt_inv',  # 'none', 'inv', or 'sqrt_inv'\n",
    "    'label_smoothing': 0.0,\n",
    "    'focal_gamma': 2.0,\n",
    "    \n",
    "    # Optimization (hardware-aware)\n",
    "    'bf16': USE_TPU or (torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    'fp16': not USE_TPU and torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "    'tf32': True,\n",
    "    'gradient_checkpointing': True,\n",
    "    \n",
    "    # Evaluation and checkpointing\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'save_strategy': 'epoch',\n",
    "    'save_total_limit': 2,\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'macro_f1',\n",
    "    \n",
    "    # Data\n",
    "    'data_dir': str(DATA_DIR),\n",
    "    'train_split': 'train',\n",
    "    'dev_split': 'dev',\n",
    "    'test_split': 'test',\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Optional: Limit samples for quick testing\n",
    "    # 'max_train_samples': 50,\n",
    "    # 'max_eval_samples': 20,\n",
    "}\n",
    "\n",
    "# Save config\n",
    "config_path = Path('/content/colab_config.yaml')\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"\u2705 Configuration saved\\n\")\n",
    "print(\"\ud83d\udccb Key settings:\")\n",
    "print(f\"   Model: {config['model_id']}\")\n",
    "print(f\"   Method: {config['method']}\")\n",
    "print(f\"   Epochs: {config['num_train_epochs']}\")\n",
    "print(f\"   Batch size: {config['per_device_train_batch_size']}\")\n",
    "print(f\"   Max length: {config['max_length']}\")\n",
    "print(f\"   Precision: {'bf16' if config['bf16'] else ('fp16' if config['fp16'] else 'fp32')}\")\n",
    "print(f\"   Device: {'TPU' if USE_TPU else 'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Start Training\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "OUTPUT_DIR = Path('/content/outputs')\n",
    "LABELS_PATH = Path('configs/labels.yaml')\n",
    "\n",
    "print(\"\ud83d\ude80 Starting training...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "!python -m src.train \\\n",
    "    --config {config_path} \\\n",
    "    --labels {LABELS_PATH} \\\n",
    "    --out_dir {OUTPUT_DIR} \\\n",
    "    --use_wandb false\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n\u2705 Training complete in {elapsed_time/60:.2f} minutes!\")\n",
    "print(f\"\ud83d\udcc1 Outputs saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results"
   },
   "outputs": [],
   "source": [
    "# Cell 8: View Training Results\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\ud83d\udcca Loading training results...\\n\")\n",
    "\n",
    "# Load metrics\n",
    "metrics_path = OUTPUT_DIR / 'metrics_dev.json'\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path) as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(\"\ud83c\udfaf Development Set Results:\")\n",
    "    print(f\"   Macro F1:    {metrics['macro_f1']:.4f}\")\n",
    "    print(f\"   Micro F1:    {metrics['micro_f1']:.4f}\")\n",
    "    print(f\"   Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
    "    \n",
    "    # Load per-label report\n",
    "    report_path = OUTPUT_DIR / 'label_report_dev.csv'\n",
    "    if report_path.exists():\n",
    "        df = pd.read_csv(report_path)\n",
    "        print(\"\\n\ud83d\udccb Per-Label Performance:\")\n",
    "        print(df[['label', 'f1', 'precision', 'recall', 'support']].to_string(index=False))\n",
    "else:\n",
    "    print(\"\u274c Metrics file not found. Training may have failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Cell 9: Evaluate on Test Set\n",
    "BEST_CKPT = OUTPUT_DIR / 'best'\n",
    "\n",
    "print(\"\ud83e\uddea Evaluating on test set...\\n\")\n",
    "\n",
    "!python -m src.eval \\\n",
    "    --ckpt {BEST_CKPT} \\\n",
    "    --labels {LABELS_PATH} \\\n",
    "    --data_dir {DATA_DIR} \\\n",
    "    --split test\n",
    "\n",
    "print(\"\\n\u2705 Test evaluation complete!\")\n",
    "\n",
    "# Load test metrics\n",
    "test_metrics_path = BEST_CKPT / 'eval_test' / 'metrics.json'\n",
    "if test_metrics_path.exists():\n",
    "    with open(test_metrics_path) as f:\n",
    "        test_metrics = json.load(f)\n",
    "    \n",
    "    print(\"\\n\ud83c\udfaf Test Set Results:\")\n",
    "    print(f\"   Macro F1:    {test_metrics['macro_f1']:.4f}\")\n",
    "    print(f\"   Micro F1:    {test_metrics['micro_f1']:.4f}\")\n",
    "    print(f\"   Weighted F1: {test_metrics['weighted_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "# Cell 10: Visualize Predictions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"\ud83d\udcc8 Generating visualizations...\\n\")\n",
    "\n",
    "# Load predictions\n",
    "pred_path = BEST_CKPT / 'eval_test' / 'predictions.csv'\n",
    "if pred_path.exists():\n",
    "    pred_df = pd.read_csv(pred_path)\n",
    "    \n",
    "    # Get label columns\n",
    "    label_cols = [\n",
    "        'depressed_mood', 'diminished_interest', 'weight_appetite_change',\n",
    "        'sleep_disturbance', 'psychomotor', 'fatigue',\n",
    "        'worthlessness_guilt', 'concentration_indecision', 'suicidality'\n",
    "    ]\n",
    "    \n",
    "    # Check if prediction columns exist\n",
    "    prob_cols = [f'{label}_prob' for label in label_cols]\n",
    "    if all(col in pred_df.columns for col in prob_cols):\n",
    "        # Plot 1: Probability distribution heatmap\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Heatmap of prediction probabilities\n",
    "        probs_matrix = pred_df[prob_cols].values[:50]  # First 50 samples\n",
    "        sns.heatmap(probs_matrix.T, ax=ax1, cmap='YlOrRd', \n",
    "                   yticklabels=[l.replace('_', ' ').title() for l in label_cols],\n",
    "                   xticklabels=False, cbar_kws={'label': 'Probability'})\n",
    "        ax1.set_title('Prediction Probabilities (First 50 samples)', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Sample Index')\n",
    "        ax1.set_ylabel('DSM-5 Symptom')\n",
    "        \n",
    "        # Plot 2: Average probability per label\n",
    "        avg_probs = pred_df[prob_cols].mean().values\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(label_cols)))\n",
    "        bars = ax2.barh([l.replace('_', ' ').title() for l in label_cols], avg_probs, color=colors)\n",
    "        ax2.set_xlabel('Average Probability', fontsize=12)\n",
    "        ax2.set_title('Average Prediction Probability by Symptom', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlim(0, 1)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, prob) in enumerate(zip(bars, avg_probs)):\n",
    "            ax2.text(prob + 0.02, i, f'{prob:.3f}', va='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot 3: Confusion-style label counts\n",
    "        if 'doc_id' in pred_df.columns:\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            # Count positive predictions per label\n",
    "            pred_counts = (pred_df[prob_cols] > 0.5).sum().values\n",
    "            \n",
    "            x = np.arange(len(label_cols))\n",
    "            ax.bar(x, pred_counts, color='steelblue', alpha=0.7, label='Predicted Positive')\n",
    "            \n",
    "            ax.set_xlabel('DSM-5 Symptom', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Predicted Positive Cases per Symptom', fontsize=14, fontweight='bold')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([l.replace('_', ' ').title() for l in label_cols], \n",
    "                              rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  Probability columns not found in predictions file\")\n",
    "else:\n",
    "    print(\"\u274c Predictions file not found\")\n",
    "\n",
    "print(\"\\n\u2705 Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export"
   },
   "outputs": [],
   "source": [
    "# Cell 11: Export Model to Drive\n",
    "from shutil import copytree, make_archive\n",
    "import os\n",
    "\n",
    "print(\"\ud83d\udcbe Exporting model...\\n\")\n",
    "\n",
    "# Option 1: Save to Google Drive (if mounted)\n",
    "if MOUNT_DRIVE:\n",
    "    DRIVE_OUTPUT = '/content/drive/MyDrive/redsm5_models/best_model'\n",
    "    os.makedirs(os.path.dirname(DRIVE_OUTPUT), exist_ok=True)\n",
    "    copytree(BEST_CKPT, DRIVE_OUTPUT, dirs_exist_ok=True)\n",
    "    print(f\"\u2705 Model saved to Google Drive: {DRIVE_OUTPUT}\")\n",
    "\n",
    "# Option 2: Create ZIP for download\n",
    "if BEST_CKPT.exists():\n",
    "    zip_path = OUTPUT_DIR / 'best_model'\n",
    "    make_archive(str(zip_path), 'zip', BEST_CKPT)\n",
    "    print(f\"\u2705 Model packaged as: {zip_path}.zip\")\n",
    "    print(f\"   Size: {os.path.getsize(str(zip_path) + '.zip') / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Uncomment to download automatically\n",
    "    # from google.colab import files\n",
    "    # files.download(str(zip_path) + '.zip')\n",
    "    # print(\"\ud83d\udce5 Download started...\")\n",
    "else:\n",
    "    print(\"\u274c Best checkpoint not found\")\n",
    "\n",
    "# Also save key artifacts\n",
    "artifacts = ['thresholds.json', 'config_used.yaml', 'metrics_dev.json', 'metrics_test.json']\n",
    "print(\"\\n\ud83d\udce6 Key artifacts:\")\n",
    "for artifact in artifacts:\n",
    "    artifact_path = OUTPUT_DIR / artifact\n",
    "    if artifact_path.exists():\n",
    "        print(f\"   \u2705 {artifact}\")\n",
    "    else:\n",
    "        print(f\"   \u26a0\ufe0f  {artifact} (not found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference"
   },
   "outputs": [],
   "source": [
    "# Cell 12: Inference Example\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"\ud83d\udd2e Loading model for inference...\\n\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BEST_CKPT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BEST_CKPT)\n",
    "model.eval()\n",
    "\n",
    "# Move to device\n",
    "if USE_TPU:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Load thresholds\n",
    "thresholds_path = OUTPUT_DIR / 'thresholds.json'\n",
    "with open(thresholds_path) as f:\n",
    "    thresholds_data = json.load(f)\n",
    "    thresholds = torch.tensor(thresholds_data['thresholds']).to(device)\n",
    "\n",
    "print(\"\u2705 Model loaded\\n\")\n",
    "\n",
    "# Define label names\n",
    "label_cols = [\n",
    "    'depressed_mood', 'diminished_interest', 'weight_appetite_change',\n",
    "    'sleep_disturbance', 'psychomotor', 'fatigue',\n",
    "    'worthlessness_guilt', 'concentration_indecision', 'suicidality'\n",
    "]\n",
    "\n",
    "def predict_symptoms(text):\n",
    "    \"\"\"Predict DSM-5 symptoms from text.\"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits).cpu()\n",
    "        preds = (probs > thresholds.cpu()).int()\n",
    "    \n",
    "    return probs[0], preds[0]\n",
    "\n",
    "# Example 1: Depressive symptoms\n",
    "print(\"\ud83d\udd0d Example 1: Depression-related text\")\n",
    "text1 = \"I feel so sad and hopeless. I can't sleep and have no energy to do anything. Nothing brings me joy anymore.\"\n",
    "probs, preds = predict_symptoms(text1)\n",
    "\n",
    "print(f\"\\nText: {text1}\")\n",
    "print(\"\\nPredicted Symptoms:\")\n",
    "for i, label in enumerate(label_cols):\n",
    "    if preds[i] == 1:\n",
    "        print(f\"  \u2713 {label.replace('_', ' ').title()} (prob: {probs[i]:.3f})\")\n",
    "\n",
    "# Example 2: Neutral text\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udd0d Example 2: Neutral text\")\n",
    "text2 = \"I went to the store today and bought some groceries. The weather was nice.\"\n",
    "probs, preds = predict_symptoms(text2)\n",
    "\n",
    "print(f\"\\nText: {text2}\")\n",
    "print(\"\\nPredicted Symptoms:\")\n",
    "detected = False\n",
    "for i, label in enumerate(label_cols):\n",
    "    if preds[i] == 1:\n",
    "        print(f\"  \u2713 {label.replace('_', ' ').title()} (prob: {probs[i]:.3f})\")\n",
    "        detected = True\n",
    "if not detected:\n",
    "    print(\"  (No symptoms detected)\")\n",
    "\n",
    "# Example 3: Custom input\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udd0d Example 3: Try your own text!\")\n",
    "print(\"\\nModify the cell below to test your own text:\")\n",
    "print(\"\"\"\\ntext_custom = \"Your text here...\"\n",
    "probs, preds = predict_symptoms(text_custom)\n",
    "# ... process results ...\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## \ud83c\udf89 Training Complete!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Improve Performance:**\n",
    "   - Use larger models (7B/13B)\n",
    "   - Increase training epochs\n",
    "   - Use real ReDSM5 data\n",
    "   - Tune hyperparameters (learning rate, batch size)\n",
    "\n",
    "2. **Experiment with Settings:**\n",
    "   - Try different pooling strategies: `'max'`, `'mean'`, `'logit_sum'`\n",
    "   - Test Focal loss: `loss_type='focal'`\n",
    "   - Adjust class weighting: `'inv'`, `'sqrt_inv'`\n",
    "   - Use QLoRA for 13B models: `method='qlora'`\n",
    "\n",
    "3. **Production Deployment:**\n",
    "   - Save best model to Google Drive\n",
    "   - Export thresholds for inference\n",
    "   - Document your results\n",
    "   - Set up monitoring\n",
    "\n",
    "4. **Analysis:**\n",
    "   - Check per-label F1 scores\n",
    "   - Analyze false positives/negatives\n",
    "   - Review threshold values\n",
    "   - Validate on held-out data\n",
    "\n",
    "### \ud83d\udcda Resources\n",
    "\n",
    "- **GitHub:** https://github.com/OscarTsao/LLM_Agents_ReDSM5\n",
    "- **Paper:** [ReDSM5 Dataset](https://arxiv.org/abs/xxxx.xxxxx)\n",
    "- **Documentation:** See `README.md` in repository\n",
    "\n",
    "### \ud83d\udca1 Tips\n",
    "\n",
    "- Use TPU for fastest training (Runtime > Change runtime type > TPU)\n",
    "- Mount Google Drive for persistent storage\n",
    "- Monitor training with wandb (`use_wandb=true`)\n",
    "- Save checkpoints regularly\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Fine-tuning! \ud83d\ude80**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ReDSM5_Training_Colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}