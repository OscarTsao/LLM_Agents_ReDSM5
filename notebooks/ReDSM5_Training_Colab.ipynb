{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReDSM5 LLM Fine-tuning on Google Colab\n",
    "\n",
    "This notebook enables training decoder-only LLMs (Llama/Qwen) with TPU/GPU acceleration on Google Colab.\n",
    "\n",
    "## Features\n",
    "- ✅ Automatic TPU/GPU/CPU detection\n",
    "- ✅ LoRA/QLoRA support for efficient fine-tuning\n",
    "- ✅ Multi-label DSM-5 symptom classification\n",
    "- ✅ Sliding window for long documents\n",
    "- ✅ Threshold optimization and model export\n",
    "\n",
    "## Before Starting\n",
    "1. Go to **Runtime > Change runtime type**\n",
    "2. Select **T4 GPU** or **TPU v2** for hardware accelerator\n",
    "3. Click **Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Installation\n",
    "!pip install -q transformers>=4.36.0 datasets>=2.16.0 accelerate>=0.25.0\n",
    "!pip install -q peft>=0.7.0 bitsandbytes>=0.41.0 scipy scikit-learn\n",
    "!pip install -q wandb optuna pyyaml pandas\n",
    "\n",
    "# Clone repository\n",
    "!git clone https://github.com/your-username/LLM_Agents_ReDSM5.git\n",
    "%cd LLM_Agents_ReDSM5\n",
    "\n",
    "print(\"✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Hardware Detection\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Try TPU detection\n",
    "try:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "    USE_TPU = True\n",
    "    print(f\"✅ TPU detected: {device}\")\n",
    "except ImportError:\n",
    "    USE_TPU = False\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✅ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"⚠️  CPU only - training will be slow\")\n",
    "\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Mount Google Drive (Optional - for data/checkpoints)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Uncomment to mount Drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_DIR = '/content/drive/MyDrive/redsm5_data'\n",
    "\n",
    "# Or use sample data\n",
    "print(\"Using local sample data for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Generate Sample Data\n",
    "from tests.fixtures.data import generate_synthetic_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Create sample dataset\n",
    "DATA_DIR = Path('/content/sample_data')\n",
    "generate_synthetic_dataset(DATA_DIR, num_samples=200, seed=42)\n",
    "\n",
    "print(f\"✅ Generated sample data in {DATA_DIR}\")\n",
    "print(f\"   Train samples: 140\")\n",
    "print(f\"   Dev samples: 30\")\n",
    "print(f\"   Test samples: 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Hugging Face Login (for gated models)\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Uncomment if using gated models like Llama-2\n",
    "# notebook_login()\n",
    "\n",
    "print(\"Skip HF login for open models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Configure Training\n",
    "import yaml\n",
    "\n",
    "config = {\n",
    "    # Model settings\n",
    "    'model_id': 'meta-llama/Llama-2-7b-hf',  # or 'Qwen/Qwen2.5-7B'\n",
    "    'method': 'lora',  # 'full_ft', 'lora', or 'qlora'\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'num_train_epochs': 3,\n",
    "    'per_device_train_batch_size': 8 if USE_TPU else 4,\n",
    "    'per_device_eval_batch_size': 16 if USE_TPU else 8,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    \n",
    "    # LoRA settings (if method='lora' or 'qlora')\n",
    "    'lora_r': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.05,\n",
    "    'lora_target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n",
    "    \n",
    "    # Document processing\n",
    "    'max_length': 2048 if USE_TPU else 1024,\n",
    "    'doc_stride': 512,\n",
    "    'truncation_strategy': 'window_pool',\n",
    "    'pooler': 'mean',  # 'max', 'mean', or 'logit_sum'\n",
    "    \n",
    "    # Loss settings\n",
    "    'loss_type': 'bce',  # or 'focal'\n",
    "    'class_weighting': 'sqrt_inv',  # 'none', 'inv', or 'sqrt_inv'\n",
    "    'label_smoothing': 0.0,\n",
    "    \n",
    "    # Optimization\n",
    "    'bf16': USE_TPU or torch.cuda.is_bf16_supported(),\n",
    "    'fp16': not USE_TPU and torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "    'tf32': True,\n",
    "    'gradient_checkpointing': True,\n",
    "    \n",
    "    # Evaluation\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'save_strategy': 'epoch',\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'macro_f1',\n",
    "    \n",
    "    # Data\n",
    "    'data_dir': str(DATA_DIR),\n",
    "    'train_split': 'train',\n",
    "    'dev_split': 'dev',\n",
    "    'test_split': 'test',\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "# Save config\n",
    "config_path = Path('/content/colab_config.yaml')\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"✅ Configuration saved\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  Model: {config['model_id']}\")\n",
    "print(f\"  Method: {config['method']}\")\n",
    "print(f\"  Epochs: {config['num_train_epochs']}\")\n",
    "print(f\"  Batch size: {config['per_device_train_batch_size']}\")\n",
    "print(f\"  Max length: {config['max_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Start Training\n",
    "OUTPUT_DIR = Path('/content/outputs')\n",
    "LABELS_PATH = Path('configs/labels.yaml')\n",
    "\n",
    "!python -m src.train \\\n",
    "    --config {config_path} \\\n",
    "    --labels {LABELS_PATH} \\\n",
    "    --out_dir {OUTPUT_DIR} \\\n",
    "    --use_wandb false\n",
    "\n",
    "print(\"\\n✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: View Training Results\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load metrics\n",
    "metrics_path = OUTPUT_DIR / 'metrics_dev.json'\n",
    "with open(metrics_path) as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"Development Set Results:\")\n",
    "print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "print(f\"  Micro F1: {metrics['micro_f1']:.4f}\")\n",
    "print(f\"  Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
    "\n",
    "# Load per-label report\n",
    "report_path = OUTPUT_DIR / 'label_report_dev.csv'\n",
    "df = pd.read_csv(report_path)\n",
    "print(\"\\nPer-Label Performance:\")\n",
    "print(df[['label', 'f1', 'precision', 'recall']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluate on Test Set\n",
    "BEST_CKPT = OUTPUT_DIR / 'best'\n",
    "\n",
    "!python -m src.eval \\\n",
    "    --ckpt {BEST_CKPT} \\\n",
    "    --labels {LABELS_PATH} \\\n",
    "    --data_dir {DATA_DIR} \\\n",
    "    --split test\n",
    "\n",
    "print(\"\\n✅ Test evaluation complete!\")\n",
    "\n",
    "# Load test metrics\n",
    "test_metrics_path = BEST_CKPT / 'eval_test' / 'metrics.json'\n",
    "with open(test_metrics_path) as f:\n",
    "    test_metrics = json.load(f)\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"  Macro F1: {test_metrics['macro_f1']:.4f}\")\n",
    "print(f\"  Micro F1: {test_metrics['micro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Visualize Predictions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load predictions\n",
    "pred_path = BEST_CKPT / 'eval_test' / 'predictions.csv'\n",
    "pred_df = pd.read_csv(pred_path)\n",
    "\n",
    "# Get label columns\n",
    "label_cols = [\n",
    "    'depressed_mood', 'diminished_interest', 'weight_appetite_change',\n",
    "    'sleep_disturbance', 'psychomotor', 'fatigue',\n",
    "    'worthlessness_guilt', 'concentration_indecision', 'suicidality'\n",
    "]\n",
    "\n",
    "# Plot label distribution\n",
    "true_cols = [f'{label}_true' for label in label_cols]\n",
    "pred_cols = [f'{label}_pred' for label in label_cols]\n",
    "\n",
    "if all(col in pred_df.columns for col in true_cols):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    true_counts = pred_df[true_cols].sum()\n",
    "    pred_counts = pred_df[pred_cols].sum()\n",
    "    \n",
    "    x = range(len(label_cols))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar([i - width/2 for i in x], true_counts, width, label='True')\n",
    "    ax.bar([i + width/2 for i in x], pred_counts, width, label='Predicted')\n",
    "    \n",
    "    ax.set_xlabel('Symptom Label')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('True vs Predicted Label Distribution')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([label.replace('_', ' ').title() for label in label_cols], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Prediction columns not found in expected format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Export Model to Drive\n",
    "from shutil import copytree\n",
    "\n",
    "# Uncomment to save to Google Drive\n",
    "# DRIVE_OUTPUT = '/content/drive/MyDrive/redsm5_models/best_model'\n",
    "# copytree(BEST_CKPT, DRIVE_OUTPUT, dirs_exist_ok=True)\n",
    "# print(f\"✅ Model saved to {DRIVE_OUTPUT}\")\n",
    "\n",
    "# Download as ZIP\n",
    "!cd {OUTPUT_DIR} && zip -r best_model.zip best/\n",
    "print(\"\\n✅ Model packaged as best_model.zip\")\n",
    "print(f\"   Location: {OUTPUT_DIR}/best_model.zip\")\n",
    "\n",
    "from google.colab import files\n",
    "# files.download(str(OUTPUT_DIR / 'best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Inference Example\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BEST_CKPT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BEST_CKPT)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load thresholds\n",
    "thresholds_path = OUTPUT_DIR / 'thresholds.json'\n",
    "with open(thresholds_path) as f:\n",
    "    thresholds_data = json.load(f)\n",
    "    thresholds = torch.tensor(thresholds_data['thresholds'])\n",
    "\n",
    "# Example text\n",
    "text = \"I feel so sad and hopeless. I can't sleep and have no energy to do anything.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits).cpu()\n",
    "    preds = (probs > thresholds).int()\n",
    "\n",
    "# Display results\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(\"Predicted Symptoms:\")\n",
    "for i, label in enumerate(label_cols):\n",
    "    if preds[0, i] == 1:\n",
    "        print(f\"  ✓ {label.replace('_', ' ').title()} (prob: {probs[0, i]:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
