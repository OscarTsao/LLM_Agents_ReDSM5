# Default training configuration for ReDSM5 multi-label classification
model_id: "meta-llama/Llama-3.1-8B"
method: "qlora"           # {full_ft|lora|qlora}
loss_type: "bce"           # {bce|focal}
class_weighting: "sqrt_inv"  # {none|inv|sqrt_inv}
optimizer: "adamw"         # {adamw|adafactor|lion}
scheduler: "cosine"        # {linear|cosine|cosine_wr|polynomial}
lr: 0.0002
weight_decay: 0.05
warmup_ratio: 0.1
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
grad_accum: 32
epochs: 4
max_steps: null
label_smoothing: 0.05
focal_gamma: 2.0
max_length: 4096
doc_stride: 512
truncation_strategy: "window_pool"  # {single|window_pool}
pooler: "max"                      # {max|mean|logit_sum}
gradient_checkpointing: true
fp16: false
bf16: true
tf32: true
use_wandb: false
wandb_project: "ReDSM5-LLM"
wandb_run_name: null
seed: 42
eval_steps: 200
logging_steps: 50
save_total_limit: 2
load_best_at_end: true
metric_for_best_model: "macro_f1"
greater_is_better: true
early_stopping_patience: 5
patience_metric: "macro_f1"
max_train_samples: null
max_eval_samples: null
max_test_samples: null
# Adapter options
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
fourbit: "nf4"       # {nf4|fp4}
double_quant: true
compute_dtype: "bf16"  # {fp16|bf16}
# Calibration options
calibration: "none"  # {none|temperature}
temperature_grid: [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]
threshold_grid_start: 0.01
threshold_grid_end: 0.99
threshold_grid_step: 0.01
# Dataset options
hf_id: ""
hf_config: ""
train_split: "train"
dev_split: "validation"
test_split: "test"
window_overlap_label_merge: "max"
# Compute resources
num_workers: 4
pin_memory: true
# Mixed precision / accelerate
sharded_ddp: false
